{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7681782,"sourceType":"datasetVersion","datasetId":4481908}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/malindaratnaduhita/sentiment-analysis-using-transformers-bert?scriptVersionId=190808043\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Sentiment Analysis in Python\nThis notebook will be using 3 different techniques:\n1. Naive Bayes\n2. BERT","metadata":{}},{"cell_type":"markdown","source":"# Read Data","metadata":{}},{"cell_type":"code","source":"#General purpose package\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-01T22:35:28.898729Z","iopub.execute_input":"2024-08-01T22:35:28.89918Z","iopub.status.idle":"2024-08-01T22:35:46.959231Z","shell.execute_reply.started":"2024-08-01T22:35:28.899144Z","shell.execute_reply":"2024-08-01T22:35:46.957844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"anies_data = pd.read_csv('/kaggle/input/indonesia-presidential-candidates-dataset-2024/Indonesia Presidential Candidates Dataset, 2024/labeled data/Anies Baswedan.csv')\nprabowo_data = pd.read_csv('/kaggle/input/indonesia-presidential-candidates-dataset-2024/Indonesia Presidential Candidates Dataset, 2024/labeled data/Prabowo Subianto.csv')\nganjar_data = pd.read_csv('/kaggle/input/indonesia-presidential-candidates-dataset-2024/Indonesia Presidential Candidates Dataset, 2024/labeled data/Ganjar Pranowo.csv')","metadata":{"execution":{"iopub.status.busy":"2024-08-01T22:35:46.962091Z","iopub.execute_input":"2024-08-01T22:35:46.963034Z","iopub.status.idle":"2024-08-01T22:35:47.297562Z","shell.execute_reply.started":"2024-08-01T22:35:46.962985Z","shell.execute_reply":"2024-08-01T22:35:47.295914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"anies_data.info()","metadata":{"execution":{"iopub.status.busy":"2024-08-01T22:35:47.306002Z","iopub.execute_input":"2024-08-01T22:35:47.306479Z","iopub.status.idle":"2024-08-01T22:35:47.35225Z","shell.execute_reply.started":"2024-08-01T22:35:47.30644Z","shell.execute_reply":"2024-08-01T22:35:47.350875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prabowo_data.info()","metadata":{"execution":{"iopub.status.busy":"2024-08-01T22:35:47.353956Z","iopub.execute_input":"2024-08-01T22:35:47.354381Z","iopub.status.idle":"2024-08-01T22:35:47.374748Z","shell.execute_reply.started":"2024-08-01T22:35:47.354332Z","shell.execute_reply":"2024-08-01T22:35:47.373232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ganjar_data.info()","metadata":{"execution":{"iopub.status.busy":"2024-08-01T22:35:47.376797Z","iopub.execute_input":"2024-08-01T22:35:47.377367Z","iopub.status.idle":"2024-08-01T22:35:47.402521Z","shell.execute_reply.started":"2024-08-01T22:35:47.377296Z","shell.execute_reply":"2024-08-01T22:35:47.400729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will combine 3 data into one.","metadata":{}},{"cell_type":"code","source":"df = pd.concat([\n    anies_data.assign(Candidate='Anies Baswedan'),\n    prabowo_data.assign(Candidate='Prabowo Subianto'),\n    ganjar_data.assign(Candidate='Ganjar Pranowo')\n])","metadata":{"execution":{"iopub.status.busy":"2024-08-01T22:35:47.404429Z","iopub.execute_input":"2024-08-01T22:35:47.404797Z","iopub.status.idle":"2024-08-01T22:35:47.423704Z","shell.execute_reply.started":"2024-08-01T22:35:47.404766Z","shell.execute_reply":"2024-08-01T22:35:47.422514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-01T22:35:47.42514Z","iopub.execute_input":"2024-08-01T22:35:47.425539Z","iopub.status.idle":"2024-08-01T22:35:47.45602Z","shell.execute_reply.started":"2024-08-01T22:35:47.425506Z","shell.execute_reply":"2024-08-01T22:35:47.454635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will using only 3 column namely Tweet Count, Text, and Label. We will also drop data that contains empty values.","metadata":{}},{"cell_type":"code","source":"df = df.loc[:, [' Tweet Count', 'Text', 'label']]\ndf = df.dropna()","metadata":{"execution":{"iopub.status.busy":"2024-08-01T22:35:47.457503Z","iopub.execute_input":"2024-08-01T22:35:47.457893Z","iopub.status.idle":"2024-08-01T22:35:47.48222Z","shell.execute_reply.started":"2024-08-01T22:35:47.457859Z","shell.execute_reply":"2024-08-01T22:35:47.480801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2024-08-01T22:35:47.487988Z","iopub.execute_input":"2024-08-01T22:35:47.488497Z","iopub.status.idle":"2024-08-01T22:35:47.510498Z","shell.execute_reply.started":"2024-08-01T22:35:47.488462Z","shell.execute_reply":"2024-08-01T22:35:47.508988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data processing","metadata":{}},{"cell_type":"code","source":"#Data processing\nimport re, string\nimport nltk\n\n#Remove punctuations, links, mentions and \\r\\n new line characters\ndef strip_all_entities(text): \n    text = text.replace('\\r', '').replace('\\n', ' ').replace('\\n', ' ').lower() #remove \\n and \\r and lowercase\n    text = re.sub(r\"(?:\\@|https?\\://)\\S+\", \"\", text) #remove links and mentions\n    text = re.sub(r'[^\\x00-\\x7f]',r'', text) #remove non utf8/ascii characters such as '\\x9a\\x91\\x97\\x9a\\x97'\n    banned_list= string.punctuation + 'Ã'+'±'+'ã'+'¼'+'â'+'»'+'§'\n    table = str.maketrans('', '', banned_list)\n    text = text.translate(table)\n    return text\n\n#clean hashtags at the end of the sentence, and keep those in the middle of the sentence by removing just the # symbol\ndef clean_hashtags(tweet):\n    new_tweet = \" \".join(word.strip() for word in re.split('#(?!(?:hashtag)\\b)[\\w-]+(?=(?:\\s+#[\\w-]+)*\\s*$)', tweet)) #remove last hashtags\n    new_tweet2 = \" \".join(word.strip() for word in re.split('#|_', new_tweet)) #remove hashtags symbol from words in the middle of the sentence\n    return new_tweet2\n\n#Filter special characters such as & and $ present in some words\ndef filter_chars(a):\n    sent = []\n    for word in a.split(' '):\n        if ('$' in word) | ('&' in word):\n            sent.append('')\n        else:\n            sent.append(word)\n    return ' '.join(sent)\n\n#Remove multiple spaces\ndef remove_mult_spaces(text): \n    return re.sub(\"\\s\\s+\" , \" \", text)","metadata":{"execution":{"iopub.status.busy":"2024-08-01T22:35:47.512149Z","iopub.execute_input":"2024-08-01T22:35:47.512538Z","iopub.status.idle":"2024-08-01T22:35:49.156322Z","shell.execute_reply.started":"2024-08-01T22:35:47.512506Z","shell.execute_reply":"2024-08-01T22:35:49.154847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_new = []\nfor t in df.Text:\n    text_new.append(remove_mult_spaces(filter_chars(clean_hashtags(strip_all_entities(t)))))\n    \ndf['clean_text'] = text_new","metadata":{"execution":{"iopub.status.busy":"2024-08-01T22:35:49.157974Z","iopub.execute_input":"2024-08-01T22:35:49.158564Z","iopub.status.idle":"2024-08-01T22:35:50.363976Z","shell.execute_reply.started":"2024-08-01T22:35:49.158517Z","shell.execute_reply":"2024-08-01T22:35:50.362546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-01T22:35:50.366324Z","iopub.execute_input":"2024-08-01T22:35:50.36683Z","iopub.status.idle":"2024-08-01T22:35:50.380854Z","shell.execute_reply.started":"2024-08-01T22:35:50.366787Z","shell.execute_reply":"2024-08-01T22:35:50.379464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we will look at the target column 'label'.","metadata":{}},{"cell_type":"code","source":"df['label'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-08-01T22:35:50.382199Z","iopub.execute_input":"2024-08-01T22:35:50.382574Z","iopub.status.idle":"2024-08-01T22:35:50.407112Z","shell.execute_reply.started":"2024-08-01T22:35:50.382544Z","shell.execute_reply":"2024-08-01T22:35:50.40567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using map function, we will convert the label Positive and Negative to 1 and 0.","metadata":{}},{"cell_type":"code","source":"df['label'] = df['label'].map({'Positive':1, 'Negative':0})","metadata":{"execution":{"iopub.status.busy":"2024-08-01T22:35:50.408838Z","iopub.execute_input":"2024-08-01T22:35:50.409231Z","iopub.status.idle":"2024-08-01T22:35:50.423586Z","shell.execute_reply.started":"2024-08-01T22:35:50.409199Z","shell.execute_reply":"2024-08-01T22:35:50.422243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['label'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-08-01T22:35:50.425101Z","iopub.execute_input":"2024-08-01T22:35:50.425531Z","iopub.status.idle":"2024-08-01T22:35:50.441058Z","shell.execute_reply.started":"2024-08-01T22:35:50.425498Z","shell.execute_reply":"2024-08-01T22:35:50.43978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that the two classes are imbalanced. To address this, we will apply oversampling to the data, which should help reduce the model’s bias towards the majority classes and improve performance on the minority classes.","metadata":{}},{"cell_type":"code","source":"from sklearn import preprocessing\nfrom imblearn.over_sampling import RandomOverSampler\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2024-08-01T22:35:50.442651Z","iopub.execute_input":"2024-08-01T22:35:50.443008Z","iopub.status.idle":"2024-08-01T22:35:50.809161Z","shell.execute_reply.started":"2024-08-01T22:35:50.442962Z","shell.execute_reply":"2024-08-01T22:35:50.807609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ros = RandomOverSampler()\ntrain_x, train_y = ros.fit_resample(np.array(df['clean_text']).reshape(-1, 1), np.array(df['label']).reshape(-1, 1));\ntrain_os = pd.DataFrame(list(zip([x[0] for x in train_x], train_y)), columns = ['clean_text', 'label']);","metadata":{"execution":{"iopub.status.busy":"2024-08-01T22:35:50.811214Z","iopub.execute_input":"2024-08-01T22:35:50.81199Z","iopub.status.idle":"2024-08-01T22:35:51.036784Z","shell.execute_reply.started":"2024-08-01T22:35:50.811954Z","shell.execute_reply":"2024-08-01T22:35:51.035682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_os['label'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-08-01T22:35:51.038642Z","iopub.execute_input":"2024-08-01T22:35:51.03904Z","iopub.status.idle":"2024-08-01T22:35:51.049487Z","shell.execute_reply.started":"2024-08-01T22:35:51.039006Z","shell.execute_reply":"2024-08-01T22:35:51.047908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train_os['clean_text'].values\ny = train_os['label'].values","metadata":{"execution":{"iopub.status.busy":"2024-08-01T22:35:51.051516Z","iopub.execute_input":"2024-08-01T22:35:51.052387Z","iopub.status.idle":"2024-08-01T22:35:51.05973Z","shell.execute_reply.started":"2024-08-01T22:35:51.052333Z","shell.execute_reply":"2024-08-01T22:35:51.058558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\nX_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-08-01T22:35:51.061095Z","iopub.execute_input":"2024-08-01T22:35:51.061516Z","iopub.status.idle":"2024-08-01T22:35:51.080578Z","shell.execute_reply.started":"2024-08-01T22:35:51.061483Z","shell.execute_reply":"2024-08-01T22:35:51.079219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will perform one-hot encoding to hopefully we will able to achieved higher accuracy. We will save a copy of the label-encoded target columns as they may be useful for future analysis.","metadata":{}},{"cell_type":"code","source":"y_train_le = y_train.copy()\ny_valid_le = y_valid.copy()\ny_test_le = y_test.copy()","metadata":{"execution":{"iopub.status.busy":"2024-08-01T22:35:51.082103Z","iopub.execute_input":"2024-08-01T22:35:51.082695Z","iopub.status.idle":"2024-08-01T22:35:51.089527Z","shell.execute_reply.started":"2024-08-01T22:35:51.082661Z","shell.execute_reply":"2024-08-01T22:35:51.088091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ohe = preprocessing.OneHotEncoder()\ny_train = ohe.fit_transform(np.array(y_train).reshape(-1, 1)).toarray()\ny_valid = ohe.fit_transform(np.array(y_valid).reshape(-1, 1)).toarray()\ny_test= ohe.fit_transform(np.array(y_test).reshape(-1, 1)).toarray()","metadata":{"execution":{"iopub.status.busy":"2024-08-01T22:35:51.091159Z","iopub.execute_input":"2024-08-01T22:35:51.091587Z","iopub.status.idle":"2024-08-01T22:35:51.114032Z","shell.execute_reply.started":"2024-08-01T22:35:51.091553Z","shell.execute_reply":"2024-08-01T22:35:51.112839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"TRAINING DATA: {X_train.shape[0]}\\nVALIDATION DATA: {X_valid.shape[0]}\\nTESTING DATA: {X_test.shape[0]}\" )","metadata":{"execution":{"iopub.status.busy":"2024-08-01T22:35:51.115447Z","iopub.execute_input":"2024-08-01T22:35:51.115816Z","iopub.status.idle":"2024-08-01T22:35:51.122908Z","shell.execute_reply.started":"2024-08-01T22:35:51.115785Z","shell.execute_reply":"2024-08-01T22:35:51.121619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Baseline model: Naive Bayes\n\nBefore implementing BERT, we will define a simple Naive Bayes baseline model to classify the tweets.\n\nFirst we need to tokenize the tweets using CountVectorizer.","metadata":{}},{"cell_type":"code","source":"#Naive Bayes\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\n\n#Metrics\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.metrics import classification_report, confusion_matrix","metadata":{"execution":{"iopub.status.busy":"2024-08-01T22:35:51.124462Z","iopub.execute_input":"2024-08-01T22:35:51.125172Z","iopub.status.idle":"2024-08-01T22:35:51.137655Z","shell.execute_reply.started":"2024-08-01T22:35:51.125125Z","shell.execute_reply":"2024-08-01T22:35:51.136135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf = CountVectorizer()\nX_train_cv =  clf.fit_transform(X_train)\nX_test_cv = clf.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-08-01T22:35:51.13958Z","iopub.execute_input":"2024-08-01T22:35:51.140109Z","iopub.status.idle":"2024-08-01T22:35:52.242859Z","shell.execute_reply.started":"2024-08-01T22:35:51.140063Z","shell.execute_reply":"2024-08-01T22:35:52.241447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then we create the TF-IDF versions of the tokenized tweets.","metadata":{}},{"cell_type":"code","source":"tf_transformer = TfidfTransformer(use_idf=True).fit(X_train_cv)\nX_train_tf = tf_transformer.transform(X_train_cv)\nX_test_tf = tf_transformer.transform(X_test_cv)","metadata":{"execution":{"iopub.status.busy":"2024-08-01T22:35:52.244662Z","iopub.execute_input":"2024-08-01T22:35:52.245083Z","iopub.status.idle":"2024-08-01T22:35:52.314002Z","shell.execute_reply.started":"2024-08-01T22:35:52.245046Z","shell.execute_reply":"2024-08-01T22:35:52.312776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we can define the Naive Bayes Classifier model.","metadata":{}},{"cell_type":"code","source":"nb_clf = MultinomialNB()\nnb_clf.fit(X_train_tf, y_train_le)","metadata":{"execution":{"iopub.status.busy":"2024-08-01T22:35:52.320738Z","iopub.execute_input":"2024-08-01T22:35:52.321163Z","iopub.status.idle":"2024-08-01T22:35:52.343603Z","shell.execute_reply.started":"2024-08-01T22:35:52.321128Z","shell.execute_reply":"2024-08-01T22:35:52.342165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nb_pred = nb_clf.predict(X_test_tf)","metadata":{"execution":{"iopub.status.busy":"2024-08-01T22:35:52.345128Z","iopub.execute_input":"2024-08-01T22:35:52.345581Z","iopub.status.idle":"2024-08-01T22:35:52.35352Z","shell.execute_reply.started":"2024-08-01T22:35:52.345545Z","shell.execute_reply":"2024-08-01T22:35:52.351968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('\\tClassification Report for Naive Bayes:\\n\\n',classification_report(y_test_le,nb_pred, target_names=['Negative', 'Positive']))","metadata":{"execution":{"iopub.status.busy":"2024-08-01T22:35:52.355048Z","iopub.execute_input":"2024-08-01T22:35:52.355521Z","iopub.status.idle":"2024-08-01T22:35:52.390738Z","shell.execute_reply.started":"2024-08-01T22:35:52.355477Z","shell.execute_reply":"2024-08-01T22:35:52.389416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model achieves an accuracy of 84%, indicating that it correctly classifies 84% of all instances. This accuracy is quite strong, suggesting the model is generally effective.\n\nIn the next section we will perform the sentiment analysis using BERT.","metadata":{}},{"cell_type":"markdown","source":"# BERT Sentiment Analysis\n\nNow we need to define a custom tokenizer function and call the encode_plus method of the BERT tokenizer.","metadata":{}},{"cell_type":"code","source":"#Transformers\nfrom transformers import BertTokenizerFast\nfrom transformers import TFBertModel\nfrom transformers import RobertaTokenizerFast\nfrom transformers import TFRobertaModel","metadata":{"execution":{"iopub.status.busy":"2024-08-01T22:35:52.392451Z","iopub.execute_input":"2024-08-01T22:35:52.392829Z","iopub.status.idle":"2024-08-01T22:35:58.907668Z","shell.execute_reply.started":"2024-08-01T22:35:52.392797Z","shell.execute_reply":"2024-08-01T22:35:58.905935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')","metadata":{"execution":{"iopub.status.busy":"2024-08-01T22:35:58.909226Z","iopub.execute_input":"2024-08-01T22:35:58.91004Z","iopub.status.idle":"2024-08-01T22:36:00.492602Z","shell.execute_reply.started":"2024-08-01T22:35:58.909998Z","shell.execute_reply":"2024-08-01T22:36:00.491147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_LEN=128","metadata":{"execution":{"iopub.status.busy":"2024-08-01T22:36:00.494487Z","iopub.execute_input":"2024-08-01T22:36:00.494839Z","iopub.status.idle":"2024-08-01T22:36:00.499987Z","shell.execute_reply.started":"2024-08-01T22:36:00.49481Z","shell.execute_reply":"2024-08-01T22:36:00.498717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenize(data,max_len=MAX_LEN) :\n    input_ids = []\n    attention_masks = []\n    for i in range(len(data)):\n        encoded = tokenizer.encode_plus(\n            data[i],\n            add_special_tokens=True,\n            max_length=MAX_LEN,\n            padding='max_length',\n            return_attention_mask=True\n        )\n        input_ids.append(encoded['input_ids'])\n        attention_masks.append(encoded['attention_mask'])\n    return np.array(input_ids),np.array(attention_masks)","metadata":{"execution":{"iopub.status.busy":"2024-08-01T22:36:00.501769Z","iopub.execute_input":"2024-08-01T22:36:00.502126Z","iopub.status.idle":"2024-08-01T22:36:00.512786Z","shell.execute_reply.started":"2024-08-01T22:36:00.502095Z","shell.execute_reply":"2024-08-01T22:36:00.511682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_input_ids, train_attention_masks = tokenize(X_train, MAX_LEN)\nval_input_ids, val_attention_masks = tokenize(X_valid, MAX_LEN)\ntest_input_ids, test_attention_masks = tokenize(X_test, MAX_LEN)","metadata":{"execution":{"iopub.status.busy":"2024-08-01T22:36:00.514444Z","iopub.execute_input":"2024-08-01T22:36:00.51496Z","iopub.status.idle":"2024-08-01T22:36:12.535366Z","shell.execute_reply.started":"2024-08-01T22:36:00.514925Z","shell.execute_reply":"2024-08-01T22:36:12.533751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_model = TFBertModel.from_pretrained('bert-base-uncased')","metadata":{"execution":{"iopub.status.busy":"2024-08-01T22:36:12.536976Z","iopub.execute_input":"2024-08-01T22:36:12.537406Z","iopub.status.idle":"2024-08-01T22:36:17.882225Z","shell.execute_reply.started":"2024-08-01T22:36:12.537369Z","shell.execute_reply":"2024-08-01T22:36:17.88084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_model(bert_model, max_len=MAX_LEN):\n    \n    ##params###\n    opt = tf.keras.optimizers.Adam(learning_rate=1e-5)\n    loss = tf.keras.losses.CategoricalCrossentropy()\n    accuracy = tf.keras.metrics.CategoricalAccuracy()\n\n\n    input_ids = tf.keras.Input(shape=(max_len,),dtype='int32')\n    \n    attention_masks = tf.keras.Input(shape=(max_len,),dtype='int32')\n    \n    embeddings = bert_model([input_ids,attention_masks])[1]\n    \n    output = tf.keras.layers.Dense(2, activation=\"softmax\")(embeddings)\n    \n    model = tf.keras.models.Model(inputs = [input_ids,attention_masks], outputs = output)\n    \n    model.compile(opt, loss=loss, metrics=accuracy)\n    \n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2024-08-01T22:36:17.884284Z","iopub.execute_input":"2024-08-01T22:36:17.885217Z","iopub.status.idle":"2024-08-01T22:36:17.895973Z","shell.execute_reply.started":"2024-08-01T22:36:17.885171Z","shell.execute_reply":"2024-08-01T22:36:17.894471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = create_model(bert_model, MAX_LEN)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2024-08-01T22:36:17.89775Z","iopub.execute_input":"2024-08-01T22:36:17.898388Z","iopub.status.idle":"2024-08-01T22:36:45.449736Z","shell.execute_reply.started":"2024-08-01T22:36:17.89822Z","shell.execute_reply":"2024-08-01T22:36:45.447816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_bert = model.fit([train_input_ids,train_attention_masks], \n                         y_train, \n                         validation_data=([val_input_ids,val_attention_masks], y_valid), \n                         epochs=2, batch_size=64)","metadata":{"execution":{"iopub.status.busy":"2024-08-01T22:36:45.451836Z","iopub.execute_input":"2024-08-01T22:36:45.45238Z","iopub.status.idle":"2024-08-02T09:35:58.062738Z","shell.execute_reply.started":"2024-08-01T22:36:45.452335Z","shell.execute_reply":"2024-08-02T09:35:58.058655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_bert = model.predict([test_input_ids,test_attention_masks])","metadata":{"execution":{"iopub.status.busy":"2024-08-02T09:35:58.069834Z","iopub.execute_input":"2024-08-02T09:35:58.070459Z","iopub.status.idle":"2024-08-02T10:06:18.653503Z","shell.execute_reply.started":"2024-08-02T09:35:58.070372Z","shell.execute_reply":"2024-08-02T10:06:18.651677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_bert =  np.zeros_like(result_bert)\ny_pred_bert[np.arange(len(y_pred_bert)), result_bert.argmax(1)] = 1","metadata":{"execution":{"iopub.status.busy":"2024-08-02T10:06:18.655694Z","iopub.execute_input":"2024-08-02T10:06:18.656104Z","iopub.status.idle":"2024-08-02T10:06:18.663929Z","shell.execute_reply.started":"2024-08-02T10:06:18.656071Z","shell.execute_reply":"2024-08-02T10:06:18.662679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Display confusion matrix\ndef conf_matrix(y, y_pred, title):\n    fig, ax =plt.subplots(figsize=(5,5))\n    labels=['Negative', 'Positive']\n    ax=sns.heatmap(confusion_matrix(y, y_pred), annot=True, cmap=\"Blues\", fmt='g', cbar=False, annot_kws={\"size\":25})\n    plt.title(title, fontsize=20)\n    ax.xaxis.set_ticklabels(labels, fontsize=17) \n    ax.yaxis.set_ticklabels(labels, fontsize=17)\n    ax.set_ylabel('Test', fontsize=20)\n    ax.set_xlabel('Predicted', fontsize=20)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-02T10:09:03.071716Z","iopub.execute_input":"2024-08-02T10:09:03.072216Z","iopub.status.idle":"2024-08-02T10:09:03.08235Z","shell.execute_reply.started":"2024-08-02T10:09:03.072179Z","shell.execute_reply":"2024-08-02T10:09:03.081013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"conf_matrix(y_test.argmax(1), y_pred_bert.argmax(1),'BERT Sentiment Analysis\\nConfusion Matrix')","metadata":{"execution":{"iopub.status.busy":"2024-08-02T10:09:03.517134Z","iopub.execute_input":"2024-08-02T10:09:03.517616Z","iopub.status.idle":"2024-08-02T10:09:03.742503Z","shell.execute_reply.started":"2024-08-02T10:09:03.517564Z","shell.execute_reply":"2024-08-02T10:09:03.74109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('\\tClassification Report for BERT:\\n\\n',classification_report(y_test,y_pred_bert, target_names=['Negative', 'Positive']))","metadata":{"execution":{"iopub.status.busy":"2024-08-02T10:09:06.255996Z","iopub.execute_input":"2024-08-02T10:09:06.256477Z","iopub.status.idle":"2024-08-02T10:09:06.293004Z","shell.execute_reply.started":"2024-08-02T10:09:06.256437Z","shell.execute_reply":"2024-08-02T10:09:06.291518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The BERT-based model achieved strong performance across the 'Negative' and 'Positive' sentiment classes.\n* Precision scores were notably high, with 92% for 'Negative' and 94% for 'Positive', meaning it accurately classified most predictions into these sentiment categories.\n* Recall scores of 94% for 'Negative' and 92% for 'Positive' demonstrate the model's ability to capture a high proportion of true instances for each sentiment class.\n* F1-scores, which harmoniously balance precision and recall, were consistent at 93% for both sentiment categories.\n* Supported by a dataset that included 4281 'Negative' and 4381 'Positive' instances, the model's overall metrics—micro, macro, weighted, and samples average—averaged at 93%.\n\nThese results underscore the model's effectiveness in accurately predicting sentiment categories based on its training and validation with the given dataset.","metadata":{}}]}